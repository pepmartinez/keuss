<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">2 posts tagged with &quot;mongodb&quot; | Keuss Job Queues</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://pepmartinez.github.io/keuss/blog/tags/mongodb"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="2 posts tagged with &quot;mongodb&quot; | Keuss Job Queues"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/keuss/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://pepmartinez.github.io/keuss/blog/tags/mongodb"><link data-rh="true" rel="alternate" href="https://pepmartinez.github.io/keuss/blog/tags/mongodb" hreflang="en"><link data-rh="true" rel="alternate" href="https://pepmartinez.github.io/keuss/blog/tags/mongodb" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/keuss/blog/rss.xml" title="Keuss Job Queues RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/keuss/blog/atom.xml" title="Keuss Job Queues Atom Feed"><link rel="stylesheet" href="/keuss/assets/css/styles.248b806b.css">
<link rel="preload" href="/keuss/assets/js/runtime~main.d957ac72.js" as="script">
<link rel="preload" href="/keuss/assets/js/main.48ea6b24.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/keuss/"><div class="navbar__logo"><img src="/keuss/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/keuss/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Keuss job queues</b></a><a class="navbar__item navbar__link" href="/keuss/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/keuss/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pepmartinez/keuss" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/keuss/blog/2024/06/20/postgresql-backend">Supporting queues on Postgresql</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/keuss/blog/2024/01/23/queues-on-mongo-part-2">Modelling queues on MongoDB - II</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/keuss/blog/2024/01/22/queues-on-mongo-part-1">Modelling queues on MongoDB - I</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/keuss/blog/2020/08/04/welcome">New website!</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;mongodb&quot;</h1><a href="/keuss/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/keuss/blog/2024/01/23/queues-on-mongo-part-2">Modelling queues on MongoDB - II</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-01-23T00:00:00.000Z" itemprop="datePublished">January 23, 2024</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/pepmartinez" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Pep Martinez</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>This is a continuation of <a href="/keuss/blog/2024/01/22/queues-on-mongo-part-1">Modelling queues on MongoDB - I</a>, where
we explained the technological basis on how to build a rather decent queue middleware by leveraging on preexisting
DB technologies, and adding very little more</p><p>Now, we explore how to push the technology further, building on top of what we got so far to add extra, useful
features</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adding-delayschedule">Adding delay/schedule<a href="#adding-delayschedule" class="hash-link" aria-label="Direct link to Adding delay/schedule" title="Direct link to Adding delay/schedule">​</a></h2><p>This is a feature that is seldom found on QMWs, but that should be easy to implement if the
persistence is sound: after all, if you got the items safely stored, they can remain stored for
any arbitrary period of time</p><p>The tricky part is to provide this feature while honoring these conditions:</p><ul><li>performance should not degrade. Both push and pop should remain <code>O(1)</code></li><li>items awaiting should not block items that are ready</li></ul><p>On the other hand, this feature can be used to implement quite a lot of common logic, so it
<em>should</em> be high in the wishlist. Some examples are:</p><ul><li><a href="https://en.wikipedia.org/wiki/Exponential_backoff" target="_blank" rel="noopener noreferrer">exponential backoff</a> if whatever you do
with an item goes wrong and you want to retry later</li><li>simple scheduling of events or actions (<em>items</em> would model both)</li><li>with some extra logic, it&#x27;s easy to build a recurring or cron-like system, where items <em>happen</em>
periodically</li></ul><p>As it turns out, this is quite easy to model on MongoDB while still maintaining all the features
and capabilities of the <em>good enough queues</em> depicted before. The model can be expressed as:</p><table><thead><tr><th align="center">operation</th><th align="center">implementation base</th></tr></thead><tbody><tr><td align="center">push</td><td align="center"><code>coll.insertOne ({payload: params.item, when: params.when OR now()})</code></td></tr><tr><td align="center">pop</td><td align="center"><code>coll.findOneAndDelete({when &lt; now()}, {orderby: {when: $asc}}).payload</code></td></tr></tbody></table><p>One of the obvious changes is, we no longer insert the item as is: we encapsulate it inside an <em>envelope</em> where we put extra information; in this case, a timestamp stating when the object should start being eligible for a <code>pop</code> operation. Thus, the <code>pop</code> will only affect items whose <code>when</code> timestamp lies in the past, and ignore those with the timestamp still in the future</p><p>Then, in order to keep the performance close to <code>O(1)</code> we must be sure the collection has an index on <code>when</code>; moreover, it would be advisable to also order the <code>findOneAndDelete</code> operation by <code>when</code>, ascending: this way we will add best-effort ordering, where elements with a longer-due timestamp are popped first</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adding-reserve-commit-rollback">Adding reserve-commit-rollback<a href="#adding-reserve-commit-rollback" class="hash-link" aria-label="Direct link to Adding reserve-commit-rollback" title="Direct link to Adding reserve-commit-rollback">​</a></h2><p>A feature that should be offered on every decent QMW is the
ability to reserve an item, then process it and commit it once
done, or rollback it if something fails and we want it to be
retried later (or by other consumer)</p><p>This allows for what&#x27;s known as <em>at-least-once</em> semantics:
every item in the queue is guaranteed to be treated at least
once even in the event of consumer failure. It <em>does not</em>
guarantee lack of duplications, though. By contrast, the simple <em>pop</em> model provides <em>at_most_once</em> semantics: duplications are
guaranteed to not to happen, but at the cost of risk of item
loss if a consumer malfunctions</p><p>Reserve-commit-rollback model can be expressed as the following extension of the
<em>delay/schedule</em> model above :</p><table><thead><tr><th align="center">operation</th><th align="center">implementation base</th></tr></thead><tbody><tr><td align="center">push</td><td align="center"><code>coll.insertOne ({payload: params.item, when: params.when OR now(), retries: 0, reserved: false})</code></td></tr><tr><td align="center">pop</td><td align="center"><code>coll.findOneAndDelete({when &lt; now()}, {orderby: {when: $asc}}).payload</code></td></tr><tr><td align="center">reserve</td><td align="center"><code>coll.findOneAndUpdate({when &lt; now()}, {when: (now() + params.timeout), reserved: true}, {orderby: {when: $asc}})</code></td></tr><tr><td align="center">commit</td><td align="center"><code>coll.delete({_id: params.reserved_item._id})</code></td></tr><tr><td align="center">rollback</td><td align="center"><code>coll.findOneAndUpdate({_id: params.reserved_item._id}, {when: (now() + params.delay), reserved: false, retries: $inc})</code></td></tr></tbody></table><p>The general idea is to leverage the existing scheduling feature: to reserve an element is just to set its <code>when</code>
time ahead in the future, by a fixed <code>timeout</code> amount; if the consumer is unable to process the element in this
time, the item will become eligible again for other consumers.</p><p>The <code>commit</code> operation simply deletes the entry by using the <code>_id</code> of the element returned by
<code>reserve</code> (which is referred to above as <code>params_reserved_item</code>); the <code>rollback</code> is a bit more complex:
it modifies it to remove the <code>reserved</code> flag, increments
the <code>retries</code> counter and -most important- sets a <code>when</code> time further in the future. This last bit fulfills
the important feature of adding delays to retries, so an element rejected by a consumer for further retry
will not be available immediately (when it is likely to fail again)</p><p>Note that the <code>reserved</code> flag is purely informational, although further checks could be done on it to improve
robustness. The same goes for <code>retries</code>: it just counts the number of retries; more logic could be added to this,
for example adding a <em>deadletter-queue</em> feature: if the number of retries goes too high, the items are moved to a
separated queue for a more dedicated processing at a later time</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="queues-with-historic-data">Queues with historic data<a href="#queues-with-historic-data" class="hash-link" aria-label="Direct link to Queues with historic data" title="Direct link to Queues with historic data">​</a></h2><p>Here&#x27;s another twist: instead of fully removing items once consumed (by means f <code>pop</code> or <code>commit</code>), we just mark
them as deleted; then we keep them around for some time, just in case we need to inspect past traffic, or replay
some items. This feature can be desirable on environments where the ability to inspect or even reproduce past traffic
is paramount. Also, this can be easily done at the expense of storage space only, with the following variation over
the model above:</p><table><thead><tr><th align="center">operation</th><th align="center">implementation base</th></tr></thead><tbody><tr><td align="center">push</td><td align="center"><code>coll.insertOne ({payload: params.item, when: params.when OR now(), retries: 0, reserved: false})</code></td></tr><tr><td align="center">pop</td><td align="center"><code>coll.findOneAndUpdate({when &lt; now(), processed: $nonexistent}, {processed: now(), when: $INFINITE}, {orderby: {when: $asc}}).payload</code></td></tr><tr><td align="center">reserve</td><td align="center"><code>coll.findOneAndUpdate({when &lt; now(), processed: $nonexistent}, {when: (now() + params.timeout), reserved: true}, {orderby: {when: $asc}})</code></td></tr><tr><td align="center">commit</td><td align="center"><code>coll.update({_id: params.reserved._id}, {processed: now(), when: $INFINITE})</code></td></tr><tr><td align="center">rollback</td><td align="center"><code>coll.findOneAndUpdate({_id: params.reserved._id}, {when: (now() + params.delay), reserved: false, retries: $inc})</code></td></tr></tbody></table><p>Then, we need to add a <a href="https://www.mongodb.com/docs/manual/core/index-ttl/" target="_blank" rel="noopener noreferrer">TTL index</a> on the new field <code>processed</code>, with
some long-enough expiration time</p><p>The main difference is the addition of a <code>processed</code> field that marks both whether the item was processed (that is <em>deleted</em>,
<em>no more</em>, <em>gone to meet its maker</em>) and if so, when that happened. This field is also used to delete old entries, once some
fixed time has elapsed. This means those queues can potentially grow very big, cause the condition to remove old entries is
age, and not size</p><p>Note that, in order to improve performance a bit, when an element is processed (after either <em>pop</em> or <em>commit</em>) its <em>when</em> is
set to some time far in the future (to <code>$INFINITE</code> and beyond, althoug in practice <code>$INFINITE</code> would be the largest date possible
and not a real infinity), to move it &#x27;away&#x27; of the <em>get</em>/<em>reserve</em> query</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="queues-fit-for-etl-pipelines-moving-elements-from-one-queue-to-the-next-atomically">Queues fit for ETL pipelines: moving elements from one queue to the next, atomically<a href="#queues-fit-for-etl-pipelines-moving-elements-from-one-queue-to-the-next-atomically" class="hash-link" aria-label="Direct link to Queues fit for ETL pipelines: moving elements from one queue to the next, atomically" title="Direct link to Queues fit for ETL pipelines: moving elements from one queue to the next, atomically">​</a></h2><p>This is an interesting concept: one of the common uses of job queues is to build what&#x27;s known as ELT pipelines: a set of
computing stations where items are transformed or otherwise processed, connected with queues. A common example would be
a POSIX shell pipeline, where several commands are tied together so the output of one becomes the input of the next; a
ETL pipeline can have also forks and loops, so the topology can be generalized to a graph, not just a linear pipeline</p><p>Let us assume for a moment that messages are never created or duplicated in any station: in other words, an item entering
a station will produce zero or one items as output. In this scenario, one of the reliability problems that arise is that,
usually, moving items from one (input) queue to the next (output) queue is not an atomic operation. This may lead to either item loss or item duplication in the case of station
malfunction, even if we use <code>reserve-commit</code></p><p>If we push to output after committing on input, we incur on risk of loss:</p><p>whereas if we push to output <em>before</em> commit on input, we risk duplication:</p><p>So, the <em>commit-in-input</em> and <em>push-on-output</em> operations must be done atomically; and it turns out it is quite simple
to extend the model to accommodate that as a new, atomic <em>move-to-queue</em> operation (although it comes at a price, as we
will see)</p><p>The easiest way to implement this operation is to require that <em>all</em> queues of a given pipeline are to be hosted in
the same mongodb collection; then, our item envelope grows to contain an extra field, <code>q</code>. And last, all
operations are augmented to use this new field:</p><table><thead><tr><th align="center">operation</th><th align="center">implementation base</th></tr></thead><tbody><tr><td align="center">push</td><td align="center"><code>coll.insertOne ({q: params.qname, payload: params.item, when: params.when OR now(), retries: 0, reserved: false})</code></td></tr><tr><td align="center">pop</td><td align="center"><code>coll.findOneAndDelete({q: params.qname, when &lt; now()}, {orderby: {when: $asc}}).payload</code></td></tr><tr><td align="center">reserve</td><td align="center"><code>coll.findOneAndUpdate({q: params.qname, when &lt; now()}, {when: (now() + params.timeout), reserved: true}, {orderby: {when: $asc}})</code></td></tr><tr><td align="center">commit</td><td align="center"><code>coll.delete({_id: params.reserved._id})</code></td></tr><tr><td align="center">rollback</td><td align="center"><code>coll.findOneAndUpdate({_id: params.reserved._id}, {when: (now() + params.delay), reserved: false, retries: $inc})</code></td></tr></tbody></table><p>The new operation <em>move-to-queue</em> is expected to act upon a reserved item, and can be modelled as:</p><table><thead><tr><th align="center">operation</th><th align="center">implementation base</th></tr></thead><tbody><tr><td align="center">moveToQ</td><td align="center"><code>coll.findOneAndUpdate({_id: params.reserved._id}, {q: params.new_qname, reserved: false, retries: 0})</code></td></tr></tbody></table><p>The operation is rather similar to a rollback, and it is definitely atomic</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="payload-mutability">Payload mutability<a href="#payload-mutability" class="hash-link" aria-label="Direct link to Payload mutability" title="Direct link to Payload mutability">​</a></h3><p>In any ETL worth its salt the payloads of the items being managed should be mutable: most logic would be otherwise narly impossible,
or very complicated, to express. </p><p>The operations in the model depicted above do not allow that, but there is nothing that prevents it; it can certainly be done as
part of the <code>update</code> section of the <code>moveToQ</code> and <code>commit</code> operations. It is simply not added here for clarity</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/keuss/blog/tags/mongodb">mongodb</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/keuss/blog/tags/tech">tech</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/keuss/blog/2024/01/22/queues-on-mongo-part-1">Modelling queues on MongoDB - I</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-01-22T00:00:00.000Z" itemprop="datePublished">January 22, 2024</time> · <!-- -->18 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/pepmartinez" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Pep Martinez</span></a></div></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>This is a series of articles describing the technical details on which <code>keuss</code> is based to build a rather complete
queue middleware (<code>QMW</code> henceforth) with a quite shallow layer on top of <code>MongoDB</code>. The basic approach is well known and understood, but
<code>keuss</code> goes well beyond the basic approach to provide extra functionalities</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="some-nomenclature">Some nomenclature<a href="#some-nomenclature" class="hash-link" aria-label="Direct link to Some nomenclature" title="Direct link to Some nomenclature">​</a></h2><p>Let us start establishing some common nomenclature that will appear later on:</p><ul><li><p><strong><em>job queue</em></strong>: A construct where elements can be inserted and extracted, in a FIFO (first in, first out) manner. Elements
extracted are removed from the queue and are no longer available</p></li><li><p><strong><em>queue middleware (qmw)</em></strong>: A system that provides queues and means for actors to perform as producers, consumers or both</p></li><li><p><strong><em>push</em></strong>: action of inserting an element into a queue</p></li><li><p><strong><em>pop</em></strong>: action of extracting an element from a queue</p></li><li><p><strong><em>reserve/commit/rollback</em></strong>: operations to provide more control on the extraction of elements: first the element is <em>reserved</em>,
(making it invisible by other <em>reserve</em> or <em>pop</em> operations, but still present in the queue), then once the element is processed it is <em>committed</em> (and only then the element is removed from the queue) or <em>rolledback</em> (meaning it is made eligible again for other <code>reserve</code> or <code>pop</code>, possibly after some delay); if neither <code>commit</code> or <code>rollback</code> happen after some time, an automatic <code>rollback</code> is applied</p></li><li><p><strong><em>consumer</em></strong>: an actor performing <code>pop</code> and/or <code>reserve-commit-rollback</code> operations on a queue. A queue can have zero or more concurrent consumers</p></li><li><p><strong><em>producer</em></strong>: an actor performing <code>push</code> operations on a queue. A queue can have zero or more concurrent producers</p></li><li><p><strong><em>at-most-once</em></strong>: consumer guarantee associated with the <code>pop</code> operation: since the element is first removed from the queue, and
then the consumer proceeds to process it, if the consumer dies or crashes in between, the element will be lost. That is, losses are
tolerated, but duplications are not</p></li><li><p><strong><em>at-least-once</em></strong>: consumer guarantee associated with the <code>reserve-commit-rollback</code> operations: if the consumer crashes between
<code>reserve</code> and <code>commit</code> the element will eventually be auto-rolledback and be processed again (possibly by another consumer).
Therefore, duplications are tolerated but losses are not</p></li><li><p><strong><em>exactly-once</em></strong>: theoretical consumer guarantee where no losses and no duplications can happen. It involves the use of monotonical
identifiers or window-based duplication detection, and is generally extremely complex to achieve, and almost in all cases with a
hefty performance penalty. It is almost never offered out of the box in any QMW</p></li><li><p><strong><em>deadletter queue</em></strong>: usually, there is a maximum number of times an element can be rolled back after a reserve, in order to prevent ill-formed or otherwise incorrect messages to stay forever in queues. Upon rollback, if the element has reached the maximum number of rollbacks it is removed from the queue and pushed into the deadletter queue, which is an otherwise regular queue</p></li><li><p><strong><em>ordered queue</em></strong>:  A non-FIFO queue: insertions are not done at the tail of the queue, but at any point. This means insertions are
no longer <em>O(1)</em>: depending on the technology used they can be <em>O(n)</em> or better, such as <em>O(log(n))</em> for a btree-based queue; same goes for push/reserve operations, they are no longer <em>O(1)</em></p><p>Using ordered queues on a QMW is key to implement certain operations: not only the more obvious such as delay, schedule or priorities,
but also robust and performing reserve/commit/rollback</p><p>Using a database to implement queues makes ordered queues a bliss: using a regular index is usually all you need to get
near-constant-complexity operations</p></li><li><p><strong><em>delay/schedule</em></strong>: Push operation when the element is marked not to be eligible for <code>pop</code> or <code>reserve</code> <em>before</em> a certain time.
The presence of delayed elements must not impact in any way the rest of elements (that is, the rest of elements&#x27; eligibility must not
change) or the queue itself (that is, the presence of delayed elements must not degrade the queue performance or capabilities)</p><p>This feature can be very easily implemented using an ordered queue, where the order is defined by a timestamp representing the
<em>mature</em> time: the time when the element can be popped or reserved, and not before</p><p>The delay/schedule feature can be applied also to rollbacks, since a <code>rollback</code> is conceptually a re-insertion; delays in rollbacks
provide a way to implement <a href="https://en.wikipedia.org/wiki/Exponential_backoff" target="_blank" rel="noopener noreferrer">exponential backoff</a> easily, to prevent busy   reserve-fail-rollback loops, when one element in the queue is repeatedly rolled back upon processing</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="basic-building-blocks">Basic building blocks<a href="#basic-building-blocks" class="hash-link" aria-label="Direct link to Basic building blocks" title="Direct link to Basic building blocks">​</a></h2><p>Here&#x27;s what you need to build a proper QMW:</p><ol><li><p>A <strong><em>storage subsystem</em></strong>: Data for the contents of the queues have to be stored somewhere. It has to provide:</p><ol><li><strong>Persistency</strong>: data must be stored in a permanent manner, reliably. In-memory QMW has its niche, but we will
focus on <em>persistent</em> QMWs</li><li><strong>High Availability</strong>: we do not want a hardware or network failure to take down the QMW. It should run in a <em>cluster</em>
manner, on several machines (possibly in separated geographical locations); if one of the machines fail the rest
can cope without (or with minimal) disruption</li><li><strong>Sufficient Throughput</strong>: the storage should be able to handle a high number of operations per second</li><li><strong>Low Latency</strong>: operations should be performed very fast, ideally as independent of throughput as possible</li></ol></li><li><p>An <strong><em>event bus</em></strong>: all QMW clients would need some form of central communication to be aware of certain events in the
QWM. For example, if a client is waiting for data to become available in a queue, it should be able to simply await for
an event, instead of running a poll busy-loop. Another example  of useful event is to signal whether a queue becomes
paused (since it must be paused for <em>all</em> clients)</p><p>This event bus can be a <em>pub/sub</em>, stateless bus: only connected clients are made aware of events and there is no need to save events for clients that may connect later. This simplifies the event bus by a lot</p></li></ol><p>The whole idea behind <code>keuss</code> is that all those building blocks are already available out there in the form of DataBase
systems, and all there is to add is a thin layer and a few extras.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-need-for-atomic-operations">The need for atomic operations<a href="#the-need-for-atomic-operations" class="hash-link" aria-label="Direct link to The need for atomic operations" title="Direct link to The need for atomic operations">​</a></h2><p>However, not just <em>any</em> storage (or DB, for that matter) is a good candidate to model queues: there is at least one feature
that, lest it be present, renders queue modelling very difficult if not impossible: <em>atomic modify operations</em></p><p>An atomic modify operation in the context of a storage system can be defined as the ability to perform a read and a modify
on a single record without the possibility of a second modify interfering, changing the record after the read but before
the modify (or after the modify and before the read)</p><p>If the storage system provides such primitives, it is relatively easy and simple to model queues on top of it; also, the
overall performance (throughput and latency) will greatly depend on the performance of such operation: most RDBMs can do
this by packing the read and the modify inside a <em>transaction</em>, but that usually degrades the performance greatly, to a
point where it is not viable for queue modelling</p><p>There are 2 major storage systems that provide all the needed blocks, along with atomic modifies: <code>MongoDB</code> and <code>Redis</code>.
<code>MongoDB</code> has turned out to be an almost perfect fit to back a QMW, as we shall see. <code>MongoDB</code> provides a set of atomic
operations to read and modify, and to read and remove. Those operations guarantee that the elements selected to be read
and then modified (or removed) will not be read by others until modified (or not read at all if it&#x27;s removed)</p><p><code>Redis</code> is also a good fit, but it does nor provide a good enough storage layer: it is neither persistent nor high available.
Arguably, that&#x27;s a default behaviour: <code>Redis-Cluster</code> coupled with proper persistence should in theory be up to the task.
However, this series of articles would focus on <code>MongoDB</code> only. For now, let us say that atomic operations are very easily
added to <code>Redis</code> by coding them as <code>lua</code> extensions, since all operations in <code>Redis</code> are atomic by design</p><p>In the following sections we will see how the implementations of common QMW operations can be indeed solved elegantly using
atomic operations provided by MongoDB as the underlying DB/storage</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="simple-approach-good-enough-queues">Simple approach: good enough queues<a href="#simple-approach-good-enough-queues" class="hash-link" aria-label="Direct link to Simple approach: good enough queues" title="Direct link to Simple approach: good enough queues">​</a></h2><p>There is a very simple, very common way to model queues on top of mongoDB collections. This model does not support
reserve-commit-rollback, nor it does support delay/schedule. The model can be succinctly put as:</p><table><thead><tr><th align="center">operation</th><th align="center">implementation base</th></tr></thead><tbody><tr><td align="center">push</td><td align="center"><code>coll.insertOne (item)</code></td></tr><tr><td align="center">pop</td><td align="center"><code>coll.findOneAndDelete()</code></td></tr></tbody></table><p>The key is the use of the atomic operation <code>findOneAndDelete</code>, which is the combination of a <code>findOne</code> and a <code>remove</code>, but
run in one single step. Several actors can perform concurrent <code>findOneAndDelete</code> operations without issues, and without
interfering each other</p><p>Actors can perform concurrent <code>insertOne</code> operations too, without interference; the same goes for actors performing <em>both</em>
<code>findOneAndDelete</code> and <code>insertOne</code> operations. The net result is that many consumers and producers can be served concurrently
without interferences or loss of performance, which is what one expects of any self-respecting QMW</p><p>This model provides a very simple but rather capable powerful QMW:</p><ul><li>queues are <em>mostly</em> strict FIFO (FIFO loses its strict meaning when different producers located in different machines
are inserting in the same queue, but in practical terms it usually does not matter)</li><li>we got very good persistence, as good as mongodb&#x27;s</li><li>we got very good HA:<ul><li>both consumers and producers have no state, so they can be replicated without problems</li><li>there is a practical 1:1 equivalence between queues and collections, so all the HA guarantees mongodb provides on
collections apply directly to queues</li></ul></li><li>we got more than decent performance:<ul><li>mongodb is quite performing on insertions, in the range of Khz (ie, thousands per second)</li><li>on pop operations, <code>findOneAndDelete</code> is less performing than a simple <code>remove</code> or a <code>findOne</code> but is still able to
reach Khz performance. In practice, <code>findOneAndDelete</code> is the bottleneck of this model, because it serializes calls
to <code>pop</code> within each queue</li></ul></li></ul><p>The main drawback of this model is the fact that the pop/reserve operations can only be performed in a poll loop: they
either return an element or return &#x27;no elements in queue&#x27; (or return an error), in all cases pretty much immediately.
Therefore, wait state of arbitrary duration must be inserted in the consumer loop if the operation returns &#x27;no elements in queue&#x27;:
otherwise you will get a busy loop where your pop/reserve call relentlessly return &#x27;no elements&#x27;, eating the CPU in the
process (incidentally, this is a text-book case of poll loop)</p><p>In some cases, where latencies in the range of seconds or tens of seconds are of no concern, a pool loop can be happily
used, so this makes a valid, simple and effective model, especially if you already use MongoDB. In cases where latencies
are expected to be near-realtime something better is needed</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adding-an-event-bus">Adding an event bus<a href="#adding-an-event-bus" class="hash-link" aria-label="Direct link to Adding an event bus" title="Direct link to Adding an event bus">​</a></h2><p>At this point, one of the best improvements to the model is to remove the need for poll loops; for that to happen, we
need the pop/reserve operations to &#x27;block&#x27; if there are no elements, until they are. A naïve way to do so is to add the
poll loop in the pop/reserve calls, so the caller would have the <em>illusion</em> of blocking:</p><p>As mentioned, this simply moves the pool loop inside the pop/reserve implementation, away from the user&#x27;s eyes. But it
is still a poll loop, with all its limitations. To truly remove the poll loop we need the ability to <em>wake up</em> a waiting
consumer <em>when</em> there are new elements in the queue:</p><p>This way the push-to-pop latencies are reduced to close to the theoretical minimum: any consumer would be blocked only when
they have to: when there are no elements</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="possible-implementations">Possible implementations<a href="#possible-implementations" class="hash-link" aria-label="Direct link to Possible implementations" title="Direct link to Possible implementations">​</a></h3><p>There is an obvious option for the implementation of such an event bus: a pub/sub subsystem. Pub/sub is very well understood,
it&#x27;s stateless and there are a lot of stable implementations. And more importantly, there are stable implementations <em>on top</em>
of we already use for storage of queues.</p><p>The main disadvantages of pub/sub in themselves as event bus are:</p><ul><li>they can not handle duplicates: in an HA setup all the replicas of a given client will get a separated copy of each event</li><li>they have no history: disconnected clients will miss any event published when they&#x27;re not connected</li></ul><p>But none of those is a real disadvantage for us:</p><ul><li>each client, whether a replica or not, <em>must</em> receive a copy of each event</li><li>disconnected clients do not need to receive and react to events, since they&#x27;re not dealing with queues</li></ul><p>Let&#x27;s see the most viable implementations:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="in-memory-pubsub">In-memory pub/sub<a href="#in-memory-pubsub" class="hash-link" aria-label="Direct link to In-memory pub/sub" title="Direct link to In-memory pub/sub">​</a></h4><p>This is a very simple, extremely nimble implementation of a pub/sub that works only within the same (OS) process. It&#x27;s only
meant to be used for testing. It can be also seen as the <em>canonical</em> implementation of the event bus</p><p>A very good and very simple implementation for node.js is <a href="https://www.npmjs.com/package/mitt" target="_blank" rel="noopener noreferrer">mitt</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="redis-pubsub">Redis pub/sub<a href="#redis-pubsub" class="hash-link" aria-label="Direct link to Redis pub/sub" title="Direct link to Redis pub/sub">​</a></h4><p><code>redis</code> offers a simple and very efficient <a href="https://redis.io/docs/manual/pubsub/" target="_blank" rel="noopener noreferrer">pub/sub implementation</a>, which can be used
as is. If you already use redis it&#x27;s definitely the way to go</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mongodb-capped-collection-also-a-pubsub">MongoDB capped collection (also a pub/sub)<a href="#mongodb-capped-collection-also-a-pubsub" class="hash-link" aria-label="Direct link to MongoDB capped collection (also a pub/sub)" title="Direct link to MongoDB capped collection (also a pub/sub)">​</a></h4><p>Since <code>mongoDB</code> is used to back the queues, it would be great if it could also power the rest of the needed subsystems; and it does,
with a just a little implementation work: it is relatively easy to build a pubsub on top of
<a href="https://www.mongodb.com/docs/upcoming/core/capped-collections/" target="_blank" rel="noopener noreferrer">mongoDB capped collections</a>, and there are quite a few
implementations readily available. One good example of such implementation in node.js is <a href="https://www.npmjs.com/package/@nodebb/mubsub" target="_blank" rel="noopener noreferrer">mubsub</a></p><p>Using this implementation has the added appeal of not adding any extra dependency: you can just use the same mongoDB server used for
the queues</p><p>This implementations has the added benefit of <em>history state</em>: it operates like a ring buffer, so there is the possibility of
accessing past events. However, this is not needed at all here</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="practical-considerations--improvements">Practical considerations &amp; improvements<a href="#practical-considerations--improvements" class="hash-link" aria-label="Direct link to Practical considerations &amp; improvements" title="Direct link to Practical considerations &amp; improvements">​</a></h3><p>There are a few considerations worth noting about how pubsub fits our purpose, and a few extras we can add to improve matters further</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="race-conditions">Race conditions<a href="#race-conditions" class="hash-link" aria-label="Direct link to Race conditions" title="Direct link to Race conditions">​</a></h4><p>A pubsub bus is asynchronous in nature, so under some conditions you may lose events. Take for example the case of a single client
that reinserts an element in the same queue it is consuming from: the event fired upon the insertion is produced at about the same time
the client attempts to pop another element. However, if the pop operation establishes the queue is empty (because the queue size was
not yet updated after the insert) but the event arrives before the pop operation starts waiting for them, you lose the event and you
risk waiting forever, when the queue has indeed one element</p><p>Race conditions such as this one are very hard to prevent entirely, if at all possible. For that reason it is recommended to use
a model in which race conditions do not cause major issues</p><p>In a system such as a <code>QMW</code> the problems to avoid at all costs are:</p><ul><li>loss of messages</li><li>duplication of messages</li><li>deadlocks and other forms of wait-forever conditions</li></ul><p>Race conditions on wake-up events won&#x27;t cause loss or duplication of messages, since this is guaranteed by the queue model; they can
however cause deadlocks, where a consumer is left waiting forever for an event that may never arrive</p><p>One way to remove this problem is to add a timeout and a poll loop: in the absence of events, the consumer will fallback into a poll
loop with a rather long period (this period would be the wait-for-events timeout), usually in the range of tens of seconds. With this
we change deadlocks into poll cycles, or deadlocks into increased latency for some rare cases</p><p>The consume pop loop would look like this:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">forever do:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  msg := pop_msg_from_queue()  // nonblocking operation, either returns a message or null if none available</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  if (!msg) do:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    await wake_up(insertion) or timeout(period)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    continue // a wakeup event arrived or the the timeout was reached: next loop either way</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  else</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return msg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  done</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">done</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="high-cardinality-of-events">High cardinality of events<a href="#high-cardinality-of-events" class="hash-link" aria-label="Direct link to High cardinality of events" title="Direct link to High cardinality of events">​</a></h4><p>A side effect of having &#x27;insert&#x27; events published is that subscribers must be ready to deal with potentially enormous amounts of events;
if you&#x27;re inserting messages in queues at, say 1 Khz and you have 50 consumers, you will have 50,000 individual events to deal with.
Besides, most of those events will add no information at all: once a consumer is woken up, it would not be interested in insert events until the queue is empty again; queue consumers just ignore events emitted when they are not idle, but the raw amount of events in the
bus might still need a noticeable amount of compute and I/O (especially on non-local pubsubs)</p><p>A simple way to minimize this is to simply ignore events if an equivalent one was emitted already in a short period of time; if that is
the case, the event is ignored right before the publish, and does not make it to the pubsub at all</p><p>Keuss uses exactly this strategy on all the included signal pubsubs, using a window of 50 ms: if the same event was emitted for the same queue within 50 ms in the past, it is ignored</p><h5 class="anchor anchorWithStickyNavbar_LWe7" id="drawbacks">Drawbacks<a href="#drawbacks" class="hash-link" aria-label="Direct link to Drawbacks" title="Direct link to Drawbacks">​</a></h5><p>This strategy has a notable drawback: it introduces an apparent race condition.</p><p>Take a queue with a single consumer; insert a single message in the queue, which would be immediately taken by the consumer. If the
consumer rejects the message with a zero delay, the consumer may still see zero elements in the next iteration, so it&#x27;ll block and
wait for insert events. The reject will indeed produce an insert event... which will be dropped because it&#x27;s equivalent to the first
insert event, that was emitted about the same millisecond</p><p>This will just be a nuisance, since the consumer would time out and rearm itself eventually. But be warned, this can happen on edge
cases</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adding-another-pubsub-implementation">Adding another pub/sub implementation<a href="#adding-another-pubsub-implementation" class="hash-link" aria-label="Direct link to Adding another pub/sub implementation" title="Direct link to Adding another pub/sub implementation">​</a></h4><p>The interface for pubsub in Keuss is very simple, so adding new or different implementations would be quite easy. For example, if
you already use <code>mqtt</code>, it makes sense to reuse it to power the event pubsub. This is however out of scope</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="final-thoughts">Final thoughts<a href="#final-thoughts" class="hash-link" aria-label="Direct link to Final thoughts" title="Direct link to Final thoughts">​</a></h3><p>At this point we got a rather decent QMW capable of push/pop with concurrent publishers and consumers, with persistence and HA, and
able to manage operations at Khz frequency with millisec latencies; all this with a quite simple and stateless implementation</p><p>This model can already solve a great deal of problems where persistent job queues are needed, especially if you already got MongoDB
in your mix. Also, it has 2 advantages over traditional QMWs :</p><ol><li><em>Performance</em>: This model produces great performance figures when compared with traditional QMWs with full persistence/HA activated</li><li><em>Simplicity</em>: the whole of the implementation is client side, and it is stateless and very thin.</li><li><em>Ease of debug</em>: it is very easy to <em>open the trunk</em>, peek inside and see exactly what&#x27;s in each queue, and it equally easy to tweak
and fix whatever problem you find. In some situations this is an invaluable feature</li></ol><p>However, we can <a href="/keuss/blog/2024/01/23/queues-on-mongo-part-2">do better</a>...</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/keuss/blog/tags/mongodb">mongodb</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/keuss/blog/tags/tech">tech</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Start Here</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/keuss/docs/quickstart">Quickstart</a></li><li class="footer__item"><a class="footer__link-item" href="/keuss/docs/">Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/keuss/docs/examples">Examples</a></li><li class="footer__item"><a class="footer__link-item" href="/keuss/docs/changelog">ChangeLog</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/keuss/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/pepmartinez/keuss" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024. Built with Docusaurus.</div></div></div></footer></div>
<script src="/keuss/assets/js/runtime~main.d957ac72.js"></script>
<script src="/keuss/assets/js/main.48ea6b24.js"></script>
</body>
</html>